{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NOTA**: Si detectas algún error en este Colab, pon un mensaje en el foro para que lo podamos solucionar o envía un correo."
      ],
      "metadata": {
        "id": "1H66kHdNDFw6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBZB2E4mgF_M"
      },
      "source": [
        "# 1 Pruebas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzK32LUcgIt9"
      },
      "source": [
        "Las pruebas son una herramienta esencial para validar que el código desarrollado funciona según lo esperado. En este caso, suele ser peligroso programar de manera rápida (producir mucho código) sin tener la seguridad que todo funciona correctamente, ya que se puede estar construyendo un castillo de naipes que se puede venir abajo en cualquier momento.\n",
        "\n",
        "Normalmente, suele ser buena idea el hecho de validar código según se vaya produciendo a través de un benchmark de pruebas, de manera que nuevas funcionalidades puedan validarse con este benchmark y comprobar que todo continúa siendo válido.\n",
        "\n",
        "Aunque ya lo sabrás de otros lenguajes, las pruebas o **tests unitarios** nos permiten mejorar la calidad de nuestro código ya que comprobamos el correcto funcionamiento de pequeñas partes de nuestro código (métodos o funciones). Básicamente, la idea consiste en escribir un método (p.e. `max_num_in_list()` que obtiene el valor máximo de una lista) y probar este método con varios valores para comprobar que el resultado es el esperado. Hay otros tipos de pruebas como los **tests de integración** que se encargan de combinar diversas funcionalidades de la aplicación y comprobar que en conjunto funcionan también correctamente. Una de las diferencias entre ambos tipos de tests es que encontrar errores es mucho más fácil en los **tests unitarios**, puesto que se centran en funcionalidades individuales.\n",
        "\n",
        "En nuestro caso, nos centraremos en los tests unitarios. Observa la implementación de la siguiente función:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAJ5An8tibrU"
      },
      "source": [
        "def max_num_in_list(list):\n",
        "  max = -1\n",
        "\n",
        "  if (len(list) == 0):\n",
        "    return \"undefined\"\n",
        "\n",
        "  for i in list:\n",
        "    if i > max:\n",
        "      max = i\n",
        "  return max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjetk3GaiRRm"
      },
      "source": [
        "Imaginemos que queremos probar el correcto funcionamiento de esta función. ¿Qué pruebas podríamos hacer? Pues posiblemente, podríamos probar con una lista de varios números como `[3,4,5]` donde debería devolver el 5. Como el orden no debería importar, podríamos probar también con `[4,5,3]` y con `[5,4,3]`. También podríamos probar con una lista de un elemento, como `[3]`... ah! y también una con algunos valores negativos... y también una lista vacía... Como ves, hay multitud de pruebas que podemos pensar incluso **antes si quiera** de programar la propia función.\n",
        "\n",
        "Como puedes imaginar, los beneficios de los tests unitarios son múltiples, como por ejemplo:\n",
        "* Reducen la posibilidad de arrastrar errores en futuras funcionalidades.\n",
        "* Facilitan la detección de errores.\n",
        "* Reducen el coste del cambio.\n",
        "* Agilizan el proceso de desarrollo.\n",
        "* Fuerzan a realizar un buen diseño."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEc11AZlkWbF"
      },
      "source": [
        "## 1.1 Pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PdkYJHSkZPE"
      },
      "source": [
        "Python nos ofrece diversas librerías para poder automatizar tests. Una de ellas es la librería de Python llamada **Pytest**. Esta librería es ampliamente utilizada. Tradicionalmente, la librería estándar de Python para pruebas es  **Unittest**. Esta librería forma parte del framework XUnit, que ofrece librerías para muchos lenguajes de programación y que son casi un estándar a la hora de programar tests unitarios. En concreto, Unittest está inspirado en **JUnit**. Sin embargo, el uso de Unittest es bastante engorroso y debes escribir muchas líneas para definir pruebas (algo que va en contra de la filosofía de Python). Por ello, una comunidad de desarrolladores pensó en crear una alternativa para testing más adecuada: Pytest.\n",
        "\n",
        "Esta librería no está incluida en la distribución estándar de Python, por lo que deberás instalarla desde tu entorno virtual:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip3 install pytest"
      ],
      "metadata": {
        "id": "4b2QO6P0-8eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación vamos a crear un primer testcase para la función anterior `max_num_in_list()`. Para ello, crearemos un fichero con el prefijo `test_` seguido del nombre del módulo a testear. En este caso, si hemos definido la función `max_num_in_list()` en un fichero llamado `funciones.py`, crearemos un fichero `test_funciones.py` para realizar las pruebas.\n",
        "\n",
        "Dentro de `test_funciones.py` crearemos una función llamada `test_` seguida del nombre que queramos, para definir la prueba que queremos realizar. Por ejemplo, a continuación puedes ver un ejemplo para testear el correcto funcionamiento cuando le pasamos la lista `[3,4,5]`:"
      ],
      "metadata": {
        "id": "wjukWiYuAn5I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-uBrC8ox-06"
      },
      "source": [
        "import funciones as f\n",
        "\n",
        "def test_max_num_in_list():\n",
        "    assert f.max_num_in_list([3,4,5]) == 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS5t-PNr03p-"
      },
      "source": [
        "Como puedes comprobar, se utiliza la keyword `assert` para realizar una comprobación. En esta comprobación, escribiremos lo que esperamos que suceda para definir que el test se ha pasado satisfactoriamente.\n",
        "\n",
        "Para ejecutar el test, desde la consola ejecutaríamos el siguiente comando:\n",
        "```python\n",
        "python -m pytest\n",
        "```\n",
        "\n",
        "Esto nos mostrará por pantalla un resultado similar al de la siguiente imagen. Como puedes comprobar, el test se ha pasado satisfactoriamente.\n",
        "\n",
        "<figure style=\"text-align:center\">\n",
        "  <center>\n",
        "  <img width = \"90%\" src=\"https://s3imagenes.s3.us-west-2.amazonaws.com/test1.PNG\"/>\n",
        "  <figcaption align=\"center\">Resultado correcto del test</figcaption>\n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, vamos a añadir distintas comprobaciones que podríamos hacer para validar que la función se ejecuta correctamente. Observa que podemos introducirlas todas en la misma función con distintos `assert`. No obstante, también podríamos ponerlas en funciones separadas:"
      ],
      "metadata": {
        "id": "aKtw6GzeFFs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_max_num_in_list():\n",
        "    assert f.max_num_in_list([3,4,5]) == 5\n",
        "    assert f.max_num_in_list([4,5,3]) == 5\n",
        "    assert f.max_num_in_list([5,4,3]) == 5\n",
        "    assert f.max_num_in_list([-3,-4,-5]) == -3\n",
        "    assert f.max_num_in_list([]) == \"undefined\""
      ],
      "metadata": {
        "id": "T6wfaPfiD8kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si ejecutas ahora el test, observarás que sale un error en una de las llamadas:\n",
        "\n",
        "<figure style=\"text-align:center\">\n",
        "  <center>\n",
        "  <img width = \"90%\" src=\"https://s3imagenes.s3.us-west-2.amazonaws.com/test2.PNG\"/>\n",
        "  <figcaption align=\"center\">Error en el test</figcaption>\n",
        "  </center>\n",
        "</figure>\n",
        "\n",
        "Esto nos indica que esta llamada en concreto ha emitido como resultado un `-1` y nosotros esperábamos un `-3`. Como vemos, tenemos mal la implementación de la función `max_num_in_list()` puesto que asumimos como `-1` el valor inicial de la variable `max`, algo que no funciona bien cuando le pasamos una lista de números negativos. En este caso, debemos cambiar este valor inicial en la función original:"
      ],
      "metadata": {
        "id": "bE2r1FeVGGT-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxY_29zz2bcy"
      },
      "source": [
        "def max_num_in_list(list):\n",
        "  max = -1000\n",
        "\n",
        "  if (len(list) == 0):\n",
        "    return \"undefined\"\n",
        "\n",
        "  for i in list:\n",
        "    if i > max:\n",
        "      max = i\n",
        "  return max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnU6HfbG2s4v"
      },
      "source": [
        "Si volvemos a ejecutar las pruebas, veremos que ahora se pasan todos los tests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten en cuenta que puedes crear tantas funciones como quieras, y que los `assert` los puedes utilizar para realizar distintas comprobaciones de otros tipos y esctucturas de datos. Por ejemplo, supongamos que tenemos las siguientes funciones en el fichero `funciones.py`:"
      ],
      "metadata": {
        "id": "8oIaQ9GLIEzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_true():\n",
        "    return True\n",
        "\n",
        "def return_lista():\n",
        "    return [1,2,3]\n",
        "\n",
        "def return_dict():\n",
        "    return {\"uno\":1, \"dos\":2}"
      ],
      "metadata": {
        "id": "pZUNjs4zIu5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, podemos definir distintas comprobaciones como las siguientes:"
      ],
      "metadata": {
        "id": "3t4iHWP0Iv3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_return_true():\n",
        "    assert f.return_true() == True\n",
        "\n",
        "def test_return_lista():\n",
        "    assert f.return_lista() == [1,2,3]\n",
        "\n",
        "def test_retorno_dic():\n",
        "    assert f.return_dict() == {\"uno\":1, \"dos\":2}"
      ],
      "metadata": {
        "id": "pkE65cbNI0Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhSk2BH19jpp"
      },
      "source": [
        "Además de poner un assert, también podemos realizar cualquier otro tipo de instrucción dentro de una función de test. Observa el siguiente ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM1pnPZ391td"
      },
      "source": [
        "def test_max_num_in_list():\n",
        "    lista = [3,4,5]\n",
        "    assert f.max_num_in_list(lista) == 5\n",
        "    lista = [4,5,3]\n",
        "    assert f.max_num_in_list(lista) == 5\n",
        "    lista.append(-50)\n",
        "    assert len(lista) == 4\n",
        "    assert f.max_num_in_list(lista) == 5\n",
        "    lista = []\n",
        "    assert f.max_num_in_list([]) == \"undefined\"\n",
        "    lista = [-1, -5, -15]\n",
        "    assert f.max_num_in_list(lista) == -1\n",
        "    #asumimos que tenemos una clase Prueba\n",
        "    n = f.Prueba()\n",
        "    assert n is not None\n",
        "    assert n.dato == \"uno\"\n",
        "    assert n.get_dato() == \"uno\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Excepciones"
      ],
      "metadata": {
        "id": "5f0uECOiVonf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginemos que tenemos la siguiente función que recibe un valor y dependiendo de este valor, puede devolver una excepción:"
      ],
      "metadata": {
        "id": "qPdIdYoaVsbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exception(val):\n",
        "    if val == True:\n",
        "        raise RuntimeError(\"Lanzo esta excepción\")\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "wAbmLJmsVykH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder probar el correcto funcionamiento, sería interesante comprobar que se cumplen las siguientes pruebas:\n",
        "\n",
        "1.   Que se emite una excepción `RuntimeError` cuando se recibe un `True` por parámetro.\n",
        "2.   Que se emite una excepción cuyo mensaje es `Lanzo esta excepción`.\n",
        "3.   Que no se emite una excepción cuando se recibe un `False`.\n",
        "\n",
        "Todas estas pruebas, las podríamos realizar con las siguientes funciones:"
      ],
      "metadata": {
        "id": "PCNeQmVDV5bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#comprobamos que se emite una excepción\n",
        "def test_exception():\n",
        "    with pytest.raises(RuntimeError):\n",
        "        f.exception(True) #esta llamada emite una excepción RuntimeError\n",
        "\n",
        "#comprobamos el mensaje de la excepción\n",
        "def test_exception_message():\n",
        "    with pytest.raises(RuntimeError) as exc:\n",
        "        f.exception(True)\n",
        "    assert \"Lanzo esta excepción\" == str(exc.value)\n",
        "\n",
        "#si la siguiente prueba no emite una excepción, es que ha funcionado correctamente:\n",
        "def test_no_exception():\n",
        "    f.exception(False)\n",
        "\n",
        "#podemos ser explícitos y assertar que un error se ha producido o assertar un 'False':\n",
        "def test_no_exception():\n",
        "    try:\n",
        "        f.exception(False)\n",
        "    except Exception as exc:\n",
        "        pytest.fail(\"Unexpected error\")\n",
        "        #la siguiente opción también funcionaría\n",
        "        #assert False"
      ],
      "metadata": {
        "id": "aFZweZqIWhi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, comentar que hasta ahora hemos trabajado con pequeños ejemplos de código situados en el mismo directorio. En un entorno real, es posible que no queramos ejecutar todos los tests cada vez. En este sentido, lo más habitual es crear un directorio **tests** (que actuaría como un paquete) donde podamos ubicar todos los ficheros **test_**."
      ],
      "metadata": {
        "id": "DZo6QMV54BvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Test fixtures"
      ],
      "metadata": {
        "id": "6R__nk0esWRC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bojkebf-Wjc"
      },
      "source": [
        "Una **fixture** hace referencia a instrucciones que se ejecutarán antes y después de cada método de test. Estas fixtures pueden ser compartidas por diversos métodos de test.\n",
        "\n",
        "Para definir estas fixtures, el método de test especifica como parámetro que necesita un determinado recurso. Entonces, Pytest busca una función con un decorador **@pytest.fixture** y cuyo nombre sea el mismo que el recurso demandado en el método de test. Funciona como una inyección de dependencias, en donde el método de test especifica que requiere un recurso y que estará disponible antes de ejecutar el test. Observa el siguiente ejemplo en donde definimos un método **lista** decorado como una fixture y otros dos métodos que necesitarán este recurso. En tiempo de ejecución, se inyectará este recurso como una dependencia de ambos métodos de test:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.fixture\n",
        "def lista():\n",
        "    lista = [3,4,5]\n",
        "    return lista\n",
        "\n",
        "def test_max_num_in_list(lista):\n",
        "    lista.append(7)\n",
        "    assert max(lista) == 7\n",
        "\n",
        "def test_append_and_length(lista):\n",
        "    lista.append(7)\n",
        "    assert len(lista) == 4"
      ],
      "metadata": {
        "id": "IhAzif1Qt-ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este tipo de fixture nos permite cargar el contexto de los tests, que pueden ser: inicializar variables, escribir valores en un fichero o conectarse a una base de datos. Sin embargo, también podemos utilizar la fixture para limpiar todo una vez termine cada prueba: borrar un fichero o borrar nuevas entradas que se han creado en la base de datos. Esto se consigue reemplazando el **return** por un **yield** de manera que podemos escribir código a continuación. En este caso, el código que pongamos a continuación se ejecutará después de cada método de test que utilice este recurso.\n",
        "\n",
        "En el siguiente ejemplo puedes ver como después de utilizar el recurso (en este caso, un fichero), se cierra:"
      ],
      "metadata": {
        "id": "aTODJZzgt53K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.fixture\n",
        "def fichero():\n",
        "    fich = open(\"resultados.txt\", \"a\")\n",
        "    yield fich\n",
        "    fich.close()\n",
        "\n",
        "def test_max_num_in_list(fichero):\n",
        "    lista = [3,4,5]\n",
        "    assert max(lista) == 5\n",
        "    fichero.write(\"Test max_num_in_list correcto\\n\")\n",
        "\n",
        "def test_append_and_length(fichero):\n",
        "    lista = [3,4,5]\n",
        "    lista.append(7)\n",
        "    assert len(lista) == 4\n",
        "    fichero.write(\"Test append_and_length correcto\\n\")"
      ],
      "metadata": {
        "id": "6cBg8nz4wO5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Tests parametrizados"
      ],
      "metadata": {
        "id": "uqAiz7bb5EF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los tests parametrizados hacen referencia a funciones o métodos y cada test se llama con distintos parámetros. Esto nos permite cubrir los tests sin añadir demasiado código. Vamos a suponer que tenemos los siguientes tests para comprobar la función **max_num_in_list** que obtiene el máximo de una lista:"
      ],
      "metadata": {
        "id": "b3Uu6mt-Q9WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_max_lista_ordenada():\n",
        "    assert f.max_num_in_list([3,4,5]) == 5\n",
        "\n",
        "def test_max_lista_desordenada():\n",
        "    assert f.max_num_in_list([4,5,3]) == 5\n",
        "\n",
        "def test_max_lista_negativos():\n",
        "    assert f.max_num_in_list([-3,-4,-5]) == -3\n",
        "\n",
        "def test_max_lista_vacia():\n",
        "    assert f.max_num_in_list([]) == \"undefined\""
      ],
      "metadata": {
        "id": "Hqxbcf53R1fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque el código anterior funciona correctamente, te habrás dado cuenta que hay código duplicado que se podría evitar usando tests parametrizados. Para ello, podemos definir un test genérico **test_max_lista** que recibe los parámetros necesarios para este test, que en este caso serían una lista y el resultado.\n",
        "\n",
        "Para indicar este tipo de test parametrizado utilizaremos la notación **@pytest.mark.parametrize**. Esta notación recibe como primer argumento la lista de los parámetros que espera el método de test y como segundo argumento, una lista con todas las combinaciones que queremos testear. A continuación puedes ver el test que comprobaría las cuatro pruebas anteriores, esta vez, como un test parametrizado:"
      ],
      "metadata": {
        "id": "ojVjljAPR4HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize(\"lista, resultado\",\n",
        "                         [([3,4,5], 5),\n",
        "                          ([4,5,3], 5),\n",
        "                          ([-3,-4,-5], -3),\n",
        "                          ([], \"undefined\"),\n",
        "                          ])\n",
        "def test_max_lista(lista, resultado):\n",
        "    assert f.max_num_in_list(lista) == resultado"
      ],
      "metadata": {
        "id": "kS5XUjfkTq5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGQslmTCETqU"
      },
      "source": [
        "# 2 Tarea entregable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJijoXd_EdgX"
      },
      "source": [
        "Modifica el ejemplo de vuelos para incluir los siguientes aspectos:\n",
        "* Un conjunto de **pruebas** en un fichero `test.py` que ejecute automáticamente diversos tests para comprobar las siguientes funciones:\n",
        "> * Creación de objetos `Flight`.\n",
        "> * Asignación de pasajeros.\n",
        "> * Reasignación de pasajeros.\n",
        "> * Número de asientos disponibles.\n",
        "\n",
        "* Se espera que contemples los distintos escenarios que puedan darse en cada función.\n",
        "\n",
        "* Debes crear una función de test por cada aspecto individual que quieras comprobar."
      ]
    }
  ]
}